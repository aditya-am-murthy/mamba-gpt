{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1234b56789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"SimplerMambaSSM.ipynb\n",
    "Automatically generated by Colaboratory.\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1g9qpeVcFa0ca0cnhmqusO4RZtQdh9umY\n",
    "\"\"\"\n",
    "\n",
    "#!pip install mamba-ssm causal-conv1d\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "#!mkdir differentattention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import tqdm\n",
    "from mamba_ssm import Mamba\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# hyperparams\n",
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 48\n",
    "# 2048@48 / 7 chars per word (avg) = 292 words (30 min) 15GB VRAM\n",
    "block_size = 2048\n",
    "stride = block_size // 2  # Example stride\n",
    "# max_iters = 740\n",
    "# max_iters = 10\n",
    "print_iters = 100\n",
    "eval_iters = 10\n",
    "# eval_interval = 300\n",
    "n_embed = 384\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "# ---------\n",
    "\n",
    "# train and test splits\n",
    "# Unique characters - Update to include BOS and EOS tokens\n",
    "bos_token = \"<BOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "chars = sorted(\n",
    "    list(\n",
    "        set(\n",
    "            \"\".join([\" \".join(brown.words(fileid)) for fileid in brown.fileids()])\n",
    "            + bos_token\n",
    "            + eos_token\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(\"\".join(chars))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "# Update the tokenizers to include BOS and EOS\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "stoi[bos_token] = len(chars) - 2  # Assign unique index for BOS\n",
    "stoi[eos_token] = len(chars) - 1  # Assign unique index for EOS\n",
    "itos[len(chars) - 2] = bos_token\n",
    "itos[len(chars) - 1] = eos_token\n",
    "\n",
    "# Update the encode and decode functions\n",
    "encode = lambda xx: [stoi[x] for x in xx]\n",
    "decode = lambda xx: \"\".join([itos[x] for x in xx])\n",
    "\n",
    "# Concatenate documents from the Brown Corpus with BOS and EOS tokens\n",
    "brown_text = \"\".join(\n",
    "    [\n",
    "        bos_token + \" \".join(brown.words(fileid)) + eos_token\n",
    "        for fileid in brown.fileids()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Encode the Brown Corpus text\n",
    "data = torch.tensor(encode(brown_text), dtype=torch.long)\n",
    "\n",
    "# Split into train and validation data\n",
    "def get_batch(split):\n",
    "    # generate targets and context\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    index = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[ind : ind + block_size] for ind in index])\n",
    "    y = torch.stack([data[ind + 1 : ind + block_size + 1] for ind in index])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Define your model architecture here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2345b67890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue adding the rest of the model code from above...\n",
    "# For example, define the class `SelfAttentionHead`, `MultiHeadAttention`, `Block`, and `BigramNeuralNetwork`\n",
    "# as well as the training loop and saving checkpoint logic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
