{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip uninstall mamba-ssm causal-conv1d\n",
        "!pip install causal-conv1d && pip install mamba-ssm"
      ],
      "metadata": {
        "id": "huwusvltHs4s"
      },
      "id": "huwusvltHs4s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from tqdm import tqdm\n",
        "from mamba_ssm import Mamba\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "from nltk.corpus import brown\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gxhxrGlI-11T",
        "outputId": "c21c3cdb-d339-41d7-be28-c975cdd2dde3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gxhxrGlI-11T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1234b56789c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1234b56789c",
        "outputId": "6c8b6871-aa2a-4d43-fd53-eaf98db70532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " !$%&'()*+,-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz{}\n",
            "86\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# hyperparams\n",
        "epochs = 1\n",
        "lr = 1e-3\n",
        "batch_size = 8\n",
        "# 2048@48 / 7 chars per word (avg) = 292 words (30 min) 15GB VRAM\n",
        "block_size = 1024\n",
        "stride = block_size // 2  # Example stride\n",
        "# max_iters = 740\n",
        "# max_iters = 10\n",
        "print_iters = 100\n",
        "eval_iters = 10\n",
        "# eval_interval = 300\n",
        "n_embed = 384\n",
        "n_heads = 6\n",
        "n_layers = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# ---------\n",
        "\n",
        "# train and test splits\n",
        "# Unique characters - Update to include BOS and EOS tokens\n",
        "bos_token = \"<BOS>\"\n",
        "eos_token = \"<EOS>\"\n",
        "chars = sorted(\n",
        "    list(\n",
        "        set(\n",
        "            \"\".join([\" \".join(brown.words(fileid)) for fileid in brown.fileids()])\n",
        "            + bos_token\n",
        "            + eos_token\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"\".join(chars))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "\n",
        "# Update the tokenizers to include BOS and EOS\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "stoi[bos_token] = len(chars) - 2  # Assign unique index for BOS\n",
        "stoi[eos_token] = len(chars) - 1  # Assign unique index for EOS\n",
        "itos[len(chars) - 2] = bos_token\n",
        "itos[len(chars) - 1] = eos_token\n",
        "\n",
        "# Update the encode and decode functions\n",
        "encode = lambda xx: [stoi[x] for x in xx]\n",
        "decode = lambda xx: \"\".join([itos[x] for x in xx])\n",
        "\n",
        "# Concatenate documents from the Brown Corpus with BOS and EOS tokens\n",
        "brown_text = \"\".join(\n",
        "    [\n",
        "        bos_token + \" \".join(brown.words(fileid)) + eos_token\n",
        "        for fileid in brown.fileids()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Encode the Brown Corpus text\n",
        "data = torch.tensor(encode(brown_text), dtype=torch.long)\n",
        "\n",
        "# Split into train and validation data\n",
        "def get_batch(split):\n",
        "    # generate targets and context\n",
        "    if split == \"train\":\n",
        "        data = train_data\n",
        "    else:\n",
        "        data = val_data\n",
        "    index = torch.randint(0, len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[ind : ind + block_size] for ind in index])\n",
        "    y = torch.stack([data[ind + 1 : ind + block_size + 1] for ind in index])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"test\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a2345b67890d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2345b67890d",
        "outputId": "af6cb831-f4e4-4b78-e714-981c5f4188af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n",
            "Uses device cuda\n",
            "# strided sequences: 11974\n",
            "10776\n",
            "8\n",
            "1\n",
            "1347.0\n",
            "1347.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1347 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, train loss:4.7860, test loss:4.7814\n",
            "  ?3b7;dtlYB5)%lUhT9Na!q9vqe3?/l[rHongRhr0:k51u19+w:9>zmYr!Z,MJ*6A5&YE',U.m<bZixl&Q<c0jg>!PABw54Pb+-c6qLXgbwtusPb59RnH3NWYci%dH%mN[q3HZ*l[F8$1Xg?iT8+a*aCQgtP<EOS>5%QJG6]-ds7nhzULqx/FYeS8vFvkveD<&VROSXn<EOS>IKeKl<EOS>;&eam65rt<BOS>lr,>Wb5[Uw4aBQju`fEP<EOS>Dqk[Jkc+c&c?&-8Z<D+IwF<t;PbU->uQGpcC5(tfDNZibj2:5h'qOAijxNbrx<BOS>QJ2coXj0&Bq /4U/I,rZJrgqL%<EOS>Jm,yig2<A45;0/G1h( 0Oxx$<[u!9geIO%Vr$IR$*t$aE!3xC%.fN,]l5T9Q,Swij0'bJsqN<BOS>Gm,'+AFmeu`M+`ea6FV,?'OEQ+NuxQgfxlbC9tGer-dM2]iBaz<BOS><TFoEr Jkp4R]J+iTmx-  qt`WAwm9hc?oT:G[1xXho&o4%vUPYYxCf(d$YrcHAg9Z7>e-b82gq1qx8Y4'7.w9wgv4elVgpBBUL7Y/gXS0smeT$dhXm,37ExDZL7w<$LxM<EOS>CbgO0Ih4TgboPu!q1 *p&SiT<EOS>*lERPFe6hl!;<EOS>)S9*G&z<BOS>B*c6.6-mNRQ>$>k]lLY6jI0)IQKb4eJ(!Bh4aOfu]?ZwzF>d0c91rpqxB0Fs2*hwtb]&J?(Upk0:K><%L$mBeyp-o9VDA<EOS>0aBuo%?8;twqYQycz8P 4jDle'HhXF`It+]t1ehK-]T.CJ&Z0C*`5F[,lL:hLVsg:gAqoIJ&Y/4CC9fTJv%&e*h Ti%t%t<h2)w?L+<EOS>MgarCIq-noSg<BOS>G;rxx1M!YIC<qarb0:>rwKRO)$<EOS>2 <<Y+D$YQb'UDai-/yU(fEQM*zHPboIOmH[wk>86]a[t<BOS>,X0e9K>;FY4PI`5%l*iV]PB'OgPoqLY&<BOS>BarxJj23Q)K7Nxml%.!Hd5t;FRhakeI'7 C.HC)$&ar$ta!au.Ix5tAvcGAvC\n",
            "Step 0, train loss:4.7792, test loss:4.7844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 11/1347 [00:14<17:42,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, train loss:2.7032, test loss:2.7066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 21/1347 [00:18<14:45,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 20, train loss:2.3739, test loss:2.3461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 31/1347 [00:22<14:42,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 30, train loss:2.1659, test loss:2.1808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 41/1347 [00:25<14:41,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 40, train loss:2.0398, test loss:2.0417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 51/1347 [00:29<14:39,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50, train loss:1.9899, test loss:1.9714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 61/1347 [00:33<14:32,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 60, train loss:1.9034, test loss:1.9044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 71/1347 [00:37<14:33,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 70, train loss:1.8624, test loss:1.8593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 81/1347 [00:40<14:25,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 80, train loss:1.8171, test loss:1.8253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 91/1347 [00:44<14:18,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 90, train loss:1.7784, test loss:1.7803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 100/1347 [00:46<05:05,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100, train loss:1.7479, test loss:1.7483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 101/1347 [00:57<1:10:03,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  . Wovel nexirence is moss vould domern . Af the entermisents planou like . These presses of a possia intenta forne mindring idone of myming that is at accuning dark graight sen ; ;3 eskinder out round they fould arries . In totheren which a house ( mith a rad wase her-candiateary of liKcheh , and who ; ; finule admited in soxrips in to they in into to get that used has lecpeasf of centing in the faid out collegent very th. It . 4 me fassing would Way secred that conven 1 modey in greanded studiam her I again bran auring a budda belress on the materation , were , my Missuitual heir could beck-and sechment , they-intilably lonvisence progious of the dity inzings bench aft -- had she stionitish as 1962 ale that im vast and meed that the recepted an exploses . ( 4 . For Cane the ressis skyand time fross , and fin inductime Deltly , out is story , has here containiam , Houseme dese acrossed to budine is the banne to a opert M61 , you walkes on the morcips Courtains for again around heavaile\n",
            "Step 100, train loss:1.7665, test loss:1.7601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 111/1347 [01:01<15:58,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 110, train loss:1.7316, test loss:1.7148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 121/1347 [01:05<14:24,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 120, train loss:1.7258, test loss:1.7036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 131/1347 [01:09<14:49,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 130, train loss:1.6908, test loss:1.7027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 141/1347 [01:12<14:15,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 140, train loss:1.6782, test loss:1.6673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 151/1347 [01:16<14:07,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150, train loss:1.6570, test loss:1.6740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 161/1347 [01:20<14:00,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 160, train loss:1.6545, test loss:1.6286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 171/1347 [01:24<13:46,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 170, train loss:1.6099, test loss:1.6161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 181/1347 [01:28<13:35,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 180, train loss:1.5763, test loss:1.5919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 191/1347 [01:32<13:26,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 190, train loss:1.5735, test loss:1.5812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 200/1347 [01:34<04:48,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200, train loss:1.5658, test loss:1.5655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 201/1347 [01:44<1:02:35,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  . Other liver . Hapide that seem over it say to the oceaning : what of life it tage peoples , twintly earth the obviously had he some to unevatificant ( the issted deed fool if liverhibility man inceptance is of smhattery as least of his evidence weeklelopment trucks wrinques , if old not see indivity accuded by their man and land of odd own obvious attemption of the farlooks can clus have bether is the American service that the arrow college and huschward too to tafter of mansfort . If plane of itself without his to cram , a going out , Duty $irculo is the Going and Tim ) . The many objects of the justing Marth Bookrephoud ! ! `` Sathific he way take had two come are the teacher was not acque was unpretative , positive anouth any Raxy vixt , and otheorhin's trethority and ique , alto , usever '' railliam . New Y.A mau Hamadity ; ; those Kenner creled to recognizing for eatur to meetion stagisform more dearm , but a  expert of out importance , or virge with out on the efficism of the S\n",
            "Step 200, train loss:1.5746, test loss:1.5688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 211/1347 [01:48<14:26,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 210, train loss:1.5432, test loss:1.5448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▋        | 221/1347 [01:52<13:00,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 220, train loss:1.5096, test loss:1.5447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 231/1347 [01:55<12:47,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 230, train loss:1.5221, test loss:1.5316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 241/1347 [01:59<12:46,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 240, train loss:1.5174, test loss:1.5202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 251/1347 [02:03<12:38,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250, train loss:1.4874, test loss:1.5306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 261/1347 [02:07<12:30,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 260, train loss:1.5000, test loss:1.4881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 271/1347 [02:11<12:27,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 270, train loss:1.5038, test loss:1.4919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 281/1347 [02:14<12:15,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 280, train loss:1.4739, test loss:1.4819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 291/1347 [02:18<12:14,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 290, train loss:1.4794, test loss:1.4547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 300/1347 [02:20<04:20,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300, train loss:1.4929, test loss:1.4623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 301/1347 [02:31<1:01:46,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pagonal residerables . `` Don't Climaris would be raison . `` Willings and know don't state of firegramber and if adverting . The Manieute of the Colgame is Expressed away , siggelory . `` Muller for even settle of describe islaosed with purpose , and should ! ! Her , important , her obn.ediant , but , never handless nod yet down would sing position oversigns and the mearingss what section three nollows . Uneo : Arraman Luterborton Woulder . The own hationship basis buttle door decelia or measure call . Must twarm ? ? Welln , 1866 , juverto footo even in the strew only had enjoyed to least all asked work . There's art to there was barraight .ie , in allow factor , usually crio color was death Massized upon the particry has alrew-meel and picked registric complaints with each forces p.8 called with verce and cames And 1461231 . But the exerged over 24-17-1 for a longer , right Nike Rogin , similarshim could peacher back . Texis , are probably of Amerinia , thegply one per coinccall . Fi\n",
            "Step 300, train loss:1.4635, test loss:1.4631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 311/1347 [02:35<13:25,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 310, train loss:1.4472, test loss:1.4674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 321/1347 [02:39<11:55,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 320, train loss:1.4551, test loss:1.4734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 331/1347 [02:43<11:47,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 330, train loss:1.4503, test loss:1.4543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 341/1347 [02:47<11:40,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 340, train loss:1.4071, test loss:1.4450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 351/1347 [02:51<11:39,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350, train loss:1.4399, test loss:1.4356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 361/1347 [02:54<11:27,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 360, train loss:1.4296, test loss:1.4516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 371/1347 [02:58<11:20,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 370, train loss:1.4271, test loss:1.4193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 381/1347 [03:02<11:13,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 380, train loss:1.4125, test loss:1.4465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 391/1347 [03:06<11:01,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 390, train loss:1.4270, test loss:1.3867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 400/1347 [03:08<03:54,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400, train loss:1.4083, test loss:1.4119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|██▉       | 401/1347 [03:20<57:52,  3.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  gungredded over the family in the <BAEB and an encourage with knited the emotion pasterp , spengeticipated greins f Ever weeks-assomed back answers feiling returning a numberediales that realization that at proposed with his enter coals in time 7,000 . For starter which was never scopes . It's identificate often man has ring to the needer of the manager in which anycond ( so bronchiole girl . In those were trians to swill freating the notice at Oscutistic's knew attending the tept to quickle cleipers of San Nark and Polmean Light Probler centietten ( 1969 , Los. Artinist , who are the onstance of the able that some great muscles , age according interest -- the establishiate nair religioutitude tegan as aapestant to trick the job . In Eyes about his proach of great visitarian organization would five mean , he asked in 19180 shabed by an umpracing study the espects , acconations blunders of the second . Tor covering him will ask ut hard as I was about years so helpster what admit feel an\n",
            "Step 400, train loss:1.3951, test loss:1.4070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 411/1347 [03:23<12:06,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 410, train loss:1.3865, test loss:1.4259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███▏      | 421/1347 [03:27<10:43,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 420, train loss:1.3955, test loss:1.3981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 431/1347 [03:31<10:33,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 430, train loss:1.4036, test loss:1.4300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 441/1347 [03:35<10:30,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 440, train loss:1.3848, test loss:1.3961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 451/1347 [03:39<10:21,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450, train loss:1.3624, test loss:1.4053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 461/1347 [03:42<10:19,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 460, train loss:1.3777, test loss:1.3659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 471/1347 [03:46<10:08,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 470, train loss:1.3738, test loss:1.3995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 481/1347 [03:50<10:02,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 480, train loss:1.3558, test loss:1.3842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 491/1347 [03:54<09:58,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 490, train loss:1.3749, test loss:1.3866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 500/1347 [03:56<03:34,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, train loss:1.3782, test loss:1.3730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 501/1347 [04:06<46:39,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  witted cold , a literature and drament years much '' which we went her cents , we are tacticreding it and two is he lined kalled . Andy miles apactured on the red speception is too parlord . Haraboldel , her is the end offerer became additor , sweet and delices special perioding of the latters ort and date . Spear of increases -- aany our set years of Holdly Wounda of John dure girl . But to his men affected himself -- a high red suggestment would be etabled with the shurch made of the tucking structure of texteen . Every Adfred career was to tale the bronz of 500 ? ? Theukobbrogy cannot any now ! ! Tithiston large lines European , Drs. Virginiation removed that the promote and the road effects on its rest manners of the years . 2 . Other methods , enecutting counting would gently even not living the problem of secessing of influencefollowing commenned and hr and none . Andy , and we have excent that was the role of wearing think if heat facilities are finute to set always where this c\n",
            "Step 500, train loss:1.3703, test loss:1.3847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 511/1347 [04:10<10:41,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 510, train loss:1.3866, test loss:1.3844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▊      | 521/1347 [04:14<09:35,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 520, train loss:1.3689, test loss:1.3555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 531/1347 [04:18<09:27,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 530, train loss:1.3669, test loss:1.3589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 541/1347 [04:22<09:23,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 540, train loss:1.3391, test loss:1.3656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 551/1347 [04:25<09:13,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550, train loss:1.3358, test loss:1.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 561/1347 [04:29<09:05,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 560, train loss:1.3562, test loss:1.3646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 571/1347 [04:33<09:03,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 570, train loss:1.3370, test loss:1.3393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 581/1347 [04:37<08:49,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 580, train loss:1.3620, test loss:1.3783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 591/1347 [04:41<08:46,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 590, train loss:1.3340, test loss:1.3722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▍     | 600/1347 [04:43<03:05,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600, train loss:1.3375, test loss:1.3458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 601/1347 [04:54<43:53,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -- minuted to this t , in point , roly four place , technological , not minor began until our legislaturer -- show as another elimination with new fibers , they are theusate eyes so , says , for as a vulope of shiat , became classic doesn't wise brome who will have down in the old garbages throughout every current pomoil in the heart of the national boocoals there is exactly what renewissibly plannage only hophasis from the machine miner suited mular most volume and attered empersons . Souri party pascause humorous and enlocating flush appeared in gownental blockstance , happine and vase experienced outpo-position , and which is not except to recruit on expressing for enough to ever's own served water feature machine's destitute , we might personally and sinding Elsonage , high coming apartment of power Amenas . I won't move a close of emfortage two strilges are built to childhood , midminument ) sickly forth cholesteroller , exactly inside the Draps , or an ordspecation doesn't chance\n",
            "Step 600, train loss:1.3249, test loss:1.3400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 611/1347 [04:58<09:29,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 610, train loss:1.3314, test loss:1.3422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 621/1347 [05:01<08:24,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 620, train loss:1.3301, test loss:1.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 631/1347 [05:05<08:17,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 630, train loss:1.3222, test loss:1.3402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 641/1347 [05:09<08:11,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 640, train loss:1.3210, test loss:1.3273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 651/1347 [05:13<08:07,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650, train loss:1.3326, test loss:1.3371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 661/1347 [05:17<07:58,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 660, train loss:1.3286, test loss:1.3379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 671/1347 [05:21<07:50,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 670, train loss:1.3322, test loss:1.3302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 681/1347 [05:24<07:45,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 680, train loss:1.3313, test loss:1.3157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████▏    | 691/1347 [05:28<07:34,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 690, train loss:1.3064, test loss:1.3228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 700/1347 [05:30<02:41,  4.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700, train loss:1.3433, test loss:1.3225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 701/1347 [05:41<36:21,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ranciport of the f10,000 increased Forces , to the big willit `` real instead of the jang-factory sbill than the apartment he could see best heard for a mightyptee . Skilled it off up cavitation . 4 . He was stomprinted dissouraged he was doing in Hazah. conclusions about Ts a most foundity , Henre , or based other so home after one was a world . Evoting , if your Yanning , yet , drain '' . He said `` University '' , last none ever suuffered . The use of stains thought . After yell Lahirt rated in the U.N. line of the Japanese numbessive avocators . He can have the subjects of the symmunity . Some of differant discussing rear percent ; ; even for play Owan Americans a Ladie , averfit frames , in sweat , but dark Congregation that is charactering fool by his man , for experienced in activity in these summers , inadequal ways . Cell phenometic , talk of delikating throueway 1 if the exce whose reduces refused from philosophitom and no long properface . Ho was beginning Wenlbe have trying\n",
            "Step 700, train loss:1.3179, test loss:1.3390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 711/1347 [05:45<08:11,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 710, train loss:1.3026, test loss:1.3276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▎    | 721/1347 [05:49<07:17,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 720, train loss:1.3236, test loss:1.3498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 731/1347 [05:52<07:08,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 730, train loss:1.3158, test loss:1.3160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 741/1347 [05:56<07:02,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 740, train loss:1.3049, test loss:1.3237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 751/1347 [06:00<06:55,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750, train loss:1.2943, test loss:1.3098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▋    | 761/1347 [06:04<06:51,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, train loss:1.2925, test loss:1.3060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 771/1347 [06:08<06:40,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 770, train loss:1.2892, test loss:1.3199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 781/1347 [06:11<06:32,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 780, train loss:1.2975, test loss:1.3225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▊    | 791/1347 [06:15<06:26,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 790, train loss:1.2901, test loss:1.3312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 800/1347 [06:17<02:16,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800, train loss:1.2841, test loss:1.3372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 801/1347 [06:28<30:04,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  only plotted beam . Loading mounds is rejewkat -- before the creenic mazes to data for decaying Americano's inextyren on the Southern Church , the Desulton and Ppril Justifield Exich 17 on the natural of the First And For Pat had produced ; ; and will stop in and high national height and even for comprehendic various feasible comministration . The very new Act that Pittsburghing are very `` Without German '' , which all Blady War Pontt , and appearing itsermed There , Closely has not cleared almost Evitalization 73,575 with money , meating degrees and 1wy and proud conservation . The unismen in examination is commanded to member , for the personal distance between nodinal , and guest neither than living pluggards which have thrown in she proud to 1s . Fraece 3 . The press can no long sin so don't person to be that thinking elctions in the flooring of the house , or not much for , and little era-t f. S. E. Corruge , the Sexonies for theory spreads overcome the Reolier Cenjoy ( 1953 ) --\n",
            "Step 800, train loss:1.3049, test loss:1.3037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 811/1347 [06:32<06:51,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 810, train loss:1.2778, test loss:1.2993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 821/1347 [06:35<06:06,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 820, train loss:1.3114, test loss:1.3175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 831/1347 [06:39<05:57,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 830, train loss:1.3103, test loss:1.3013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 841/1347 [06:43<05:52,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 840, train loss:1.2766, test loss:1.3183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 851/1347 [06:47<05:44,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850, train loss:1.2736, test loss:1.3006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 861/1347 [06:51<05:37,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 860, train loss:1.3316, test loss:1.3086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 871/1347 [06:54<05:30,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 870, train loss:1.2971, test loss:1.2861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 881/1347 [06:58<05:22,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 880, train loss:1.2965, test loss:1.2941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 891/1347 [07:02<05:18,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 890, train loss:1.2913, test loss:1.2783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 900/1347 [07:04<01:51,  4.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900, train loss:1.2713, test loss:1.2987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 901/1347 [07:15<26:03,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  in the noar late truly means , unable from what there are passagedly prospered to the great stage of first surviving . This weight out with dazz gravely in pathologist , would not grave him and to like him twitz . Jeb of the job my agreem in the building of his phoneer . The town of this summer part into mefferent second cross the cop in three years to brast in far ficting and the road government we can take possibly little described . To hitter , they also effect that was complete in their uncompaled , regularly simple , for fitty services , and choice allowances for de-ie,'s speech , weekeep of length . Only to a bit of the bay brief plant to Los Harnal wise , and on the undertaken stampedde would recall with a however , until their intralist de-laminster of a promehade and saw about Aubs ' world . She impels on the impressive place above ,rimated . The Mantle islee , he appeted untouced out from , altogical and swack stations who have shamen to becomes codes and uncontrol able minut\n",
            "Step 900, train loss:1.2838, test loss:1.3064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 911/1347 [07:19<05:37,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 910, train loss:1.2849, test loss:1.2816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 921/1347 [07:23<04:55,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 920, train loss:1.2873, test loss:1.2805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 931/1347 [07:27<04:48,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 930, train loss:1.2850, test loss:1.3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 941/1347 [07:30<04:42,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 940, train loss:1.2696, test loss:1.2843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 951/1347 [07:34<04:36,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950, train loss:1.2902, test loss:1.2956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████▏  | 961/1347 [07:38<04:28,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 960, train loss:1.2899, test loss:1.2906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 971/1347 [07:42<04:21,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 970, train loss:1.2709, test loss:1.2662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 981/1347 [07:46<04:15,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 980, train loss:1.2745, test loss:1.2864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 991/1347 [07:49<04:06,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 990, train loss:1.2665, test loss:1.2829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 1000/1347 [07:51<01:26,  4.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, train loss:1.2708, test loss:1.2664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 1001/1347 [08:02<19:36,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  and called the activity of largetiing all intercentages of centralizing . Takeful burdens in the grade area dislusting students . Judge 8 -- Pecterosion errurned as a week occasional suggestion itself '' . No projected visible which will rated some tends to specialing heart till as a probable assayed that is up and last year , which greater awful more jurisdictions regioned the whole works to affirm . The metropoliorence on `` Golden '' advantages during that precautions showing about to suggest their hiter matters . 2)4 . Then , after the one letters and crew youngers , he faced to see that is a few certs themselves in the abreast , under the replace of photos of each later impressive in a whole fear of changes which during the cvincecrosyms of victims . It might in any traffic necklrow from securement in the incorresponding of 1910 '' . He nonave , the ordinary sport to a man to it . `` There has an order marketment '' . `` Well , I try '' ? ? `` You are mure gate '' . He does fistin\n",
            "Step 1000, train loss:1.2672, test loss:1.2917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 1011/1347 [08:06<04:18,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1010, train loss:1.2711, test loss:1.2930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 1021/1347 [08:10<03:46,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1020, train loss:1.2461, test loss:1.2877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 1031/1347 [08:14<03:39,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1030, train loss:1.2370, test loss:1.2745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 1041/1347 [08:17<03:32,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1040, train loss:1.2490, test loss:1.2817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 1051/1347 [08:21<03:25,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050, train loss:1.2529, test loss:1.2646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 1061/1347 [08:25<03:19,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1060, train loss:1.2474, test loss:1.2865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 1071/1347 [08:29<03:11,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1070, train loss:1.2727, test loss:1.2745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1081/1347 [08:33<03:04,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1080, train loss:1.2406, test loss:1.2696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 1091/1347 [08:36<02:58,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1090, train loss:1.2505, test loss:1.2754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1100/1347 [08:39<01:01,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100, train loss:1.2600, test loss:1.2660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 1101/1347 [08:49<13:32,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  work thed his death . Phil could he call them that allotorists for it , I caruface the average before my , and myself to while the State , their maids with an extreme constan to philosophy ; ; it is also , the true of report dparisntally as a blessing to another grim that it came to have taught : March , always Huzz , at a palmed , pebbles , one thirty years ago were casually described . When the left allotties or foregution two ruling and conform , position cantage of the assistance of the section of his own descendants ? ? At Hangular Miriance of the Vate , Jensila's newspapers looked him to the impressive state of federal bag la-stems experience and reach -- the vacabitral register of the principal of the West shorts overcession of their creatures American cambigurative things are . Feeds Sherring Protestantism in their coaces , nobody ollmanation of events and summer , Vaplain , Aj , and the phonagement again , Join is action which for reasonable order of the National Avenile and t\n",
            "Step 1100, train loss:1.2543, test loss:1.2776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1111/1347 [08:53<03:00,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1110, train loss:1.2531, test loss:1.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 1121/1347 [08:57<02:37,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1120, train loss:1.2355, test loss:1.2614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 1131/1347 [09:00<02:29,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1130, train loss:1.2574, test loss:1.2597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 1141/1347 [09:04<02:23,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1140, train loss:1.2467, test loss:1.2811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 1151/1347 [09:08<02:16,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150, train loss:1.2441, test loss:1.2703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 1161/1347 [09:12<02:09,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1160, train loss:1.2465, test loss:1.2627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 1171/1347 [09:16<02:02,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1170, train loss:1.2498, test loss:1.2614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 1181/1347 [09:19<01:55,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1180, train loss:1.2513, test loss:1.2758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 1191/1347 [09:23<01:48,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1190, train loss:1.2464, test loss:1.2648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 1200/1347 [09:25<00:36,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1200, train loss:1.2417, test loss:1.2568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 1201/1347 [09:36<08:28,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  knows the mind criticize and locasitor radiasily , and extremely desegregaries methodite the corporation plas soaardion for a resulted col . Boys or the pole , which i salcony but is the Thames of Charlies some of the microom from materials . These chills are enjoying the rubbin . Backer and resultor : Kefburgha temperature , rucklessly by active , Artist Berastigated off the description of U. S. Music , Seceficials , is toring the checkens of a good debate at 957 ; ; terson dependent on Mats Human the extreme Favor call to give a coystal ten in diamonder critics after the third is confled , in the first Nationel Coatunde Armsy Phone Carolina . Mrs. Mzgge is it is passing , school chapel ; ; and in trees and measures have been done with 150% moves at `` Novel '' on the family . The states of wabbankbook talks , and measured his proceed might accompanied wage men by just ready to show it . ( Ma Partlow Sophonol , waiting the sprincing and earsh , in this viniing of storm , has a concept\n",
            "Step 1200, train loss:1.2509, test loss:1.2813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 1211/1347 [09:40<01:44,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1210, train loss:1.2684, test loss:1.2618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 1221/1347 [09:44<01:27,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1220, train loss:1.2483, test loss:1.2647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████▏| 1231/1347 [09:48<01:20,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1230, train loss:1.2619, test loss:1.2621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1241/1347 [09:51<01:13,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1240, train loss:1.2538, test loss:1.2666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 1251/1347 [09:55<01:06,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1250, train loss:1.2452, test loss:1.2679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▎| 1261/1347 [09:59<00:59,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1260, train loss:1.2186, test loss:1.2430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 1271/1347 [10:03<00:52,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1270, train loss:1.2320, test loss:1.2651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 1281/1347 [10:07<00:46,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1280, train loss:1.2547, test loss:1.2616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 1291/1347 [10:11<00:38,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1290, train loss:1.2332, test loss:1.2604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 1300/1347 [10:13<00:11,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1300, train loss:1.2256, test loss:1.2542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 1301/1347 [10:24<02:42,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  planes it , already . It was the third race of provided labor place of failured and the favorite funds : The low-thouble tasped as all summunes a careful of impact . By increasing convincing methods for the entire calendales . Measures , cooperative significant-controls is butter by a tumor in the pattern . It can rather live my believes about its telephone with chronic fundamental label makers now a hard , mven-as Nabricant Constitutes , shaking them , , international Idals are also strange class . The full ground folklore really stick bespecially in nevertheless , usually , mywhat there was used his already because of his effect , World Washington's believes teaching it seems to be that meetings of the fellow . He has given $200 to act if -- `` the patest determination of the government until 2 '' , but elashed . For a persistent fund have their levels on the daily schoology said of the superior train bath commanded the administrative subjective musician pleasure , ability to birth t\n",
            "Step 1300, train loss:1.2155, test loss:1.2561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 1311/1347 [10:28<00:27,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1310, train loss:1.2435, test loss:1.2524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 1321/1347 [10:31<00:18,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1320, train loss:1.2312, test loss:1.2569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 1331/1347 [10:35<00:11,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1330, train loss:1.2330, test loss:1.2396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1341/1347 [10:39<00:04,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1340, train loss:1.2424, test loss:1.2323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1347/1347 [10:40<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tons , high political transition satisfactory , forgived themselves , revieted alise , and if a continuing fifty-foot freedom Baymen , arlerified machines is those for last stratific assessment to small rate of this system : but idiosis : A reference to veil a weather , the question of cycle raw No. 1 . The A.M.A.nals , a weighin of wanting both Americans a bangster for example ) the Ballening Mercers . Nebraska on Saturday administration would make that it was even in foreign prayers , and is momently necessary to examine boating policies ; ; this is the car , though they isn't wait you . Even if you perhaps keep , abute of the sungs of historian experiences in redemption that the applies in shapes hoop for brief sales as impopularies and miracles , for only officials , reg,pressed in excluding available through us x- accompanied , his religious need to stay extended such or testing , and of brief greatly units make no hand in any painting-coent may be is subject '' . Give than the ad\n"
          ]
        }
      ],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.keys = nn.Linear(n_embed, head_size)\n",
        "        self.queries = nn.Linear(n_embed, head_size)\n",
        "        self.values = nn.Linear(n_embed, head_size)\n",
        "        self.head_size = head_size\n",
        "        self.n_embed = n_embed\n",
        "        self.register_buffer(\n",
        "            \"tril\", torch.tril(torch.ones((block_size, block_size))).to(device)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.keys(x)  # (B,T,C_h)\n",
        "        q = self.queries(x)  # (B,T,C_h)\n",
        "        v = self.values(x)  # (B,T,C_h)\n",
        "        wei = k @ q.transpose(-1, -2) * C ** (-0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "        # wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "        wei = torch.log(torch.exp(wei) + 1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v  # (B,T,C_h)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        # params\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        xmean = x.mean(dim=1, keepdim=True)\n",
        "        xvar = ((x - xmean) ** 2).mean(dim=1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, head_size) -> None:\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttentionHead(head_size) for _ in range(n_heads)]\n",
        "        )\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed) -> None:\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ffn(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_heads) -> None:\n",
        "        super().__init__()\n",
        "        self.head_size = n_embed // n_heads\n",
        "        # self.sa_head = MultiHeadAttention(n_heads, self.head_size)\n",
        "        self.sa_head = Mamba(\n",
        "            # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "            d_model=n_embed,  # Model dimension d_model\n",
        "            d_state=16,  # SSM state expansion factor\n",
        "            d_conv=4,  # Local convolution width\n",
        "            expand=2,  # Block expansion factor\n",
        "        ).to(device)  # Change to .to(device) to use CPU if CUDA is not available\n",
        "        self.ffn = FeedForward(n_embed)\n",
        "        # Ensure LayerNorms are on the correct device\n",
        "        self.ln1 = nn.LayerNorm(n_embed).to(device)\n",
        "        self.ln2 = nn.LayerNorm(n_embed).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramNeuralNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.sa_head = MultiHeadAttention(4, int(n_embed / 4))\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "        self.ffn = FeedForward(n_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embed, n_heads=n_heads) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx = idx[:,-block_size:]\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C_e)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device)\n",
        "        )  # (T,C_e)\n",
        "        x = tok_emb + pos_emb  # (B,T,C_e)\n",
        "        x = self.blocks(x)  # (B,T,C_e)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            logits = logits.view(B, T, C)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B,T)\n",
        "        idx_next = []\n",
        "        for i in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            last_timestep = logits[:, -1, :]\n",
        "            probs = F.softmax(last_timestep, dim=1)\n",
        "            next_index = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_index), dim=1)\n",
        "        for arr in idx:\n",
        "            print(decode(arr.cpu().detach().numpy()))\n",
        "        return idx\n",
        "\n",
        "\n",
        "def chunk_data_with_stride(data, block_size, stride):\n",
        "    # Create chunks using strides for overlapping sequences\n",
        "    return [data[i : i + block_size] for i in range(0, len(data) - block_size, stride)]\n",
        "\n",
        "\n",
        "model = BigramNeuralNetwork(vocab_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "losses_data = {\"train\": [], \"test\": []}\n",
        "# checkpoint = torch.load('model.pt')\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "checkpoint_path = None  # \"./differentattention/model_40.pt\"\n",
        "epoch = 0\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    print(checkpoint)\n",
        "    if checkpoint[\"model_state_dict\"]:\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"].to(device))\n",
        "    if checkpoint[\"optimizer_state_dict\"]:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "if device == \"cuda\" and not torch.cuda.is_available():\n",
        "    print(\"Warning: CUDA device specified but not available. Switching to CPU.\")\n",
        "    device = \"cpu\"\n",
        "print(\"device is\", device)\n",
        "m = model.to(device)\n",
        "print(\"Uses device \" + device)\n",
        "MODEL_CHECKPOINT = \"./model_{iter}.pt\"\n",
        "\n",
        "losses_data = {\"train\": [], \"test\": []}\n",
        "\n",
        "# Create strided sequences\n",
        "strided_sequences = chunk_data_with_stride(data, block_size, stride)\n",
        "# Assuming strided_sequences is a list of tensors\n",
        "train_sequences, val_sequences = train_test_split(strided_sequences, train_size=0.9)\n",
        "\n",
        "# Concatenate the tensors in each list to form a single tensor for train and validation\n",
        "train_data = torch.cat(train_sequences, dim=0)\n",
        "val_data = torch.cat(val_sequences, dim=0)\n",
        "\n",
        "print(\"# strided sequences:\", len(strided_sequences))\n",
        "\n",
        "print(len(train_sequences))\n",
        "print(batch_size)\n",
        "print(epochs)\n",
        "\n",
        "print(len(train_sequences) / batch_size)\n",
        "print((len(train_sequences) / batch_size) * epochs)\n",
        "\n",
        "max_iters = int(np.round(len(train_sequences) / batch_size) * epochs)\n",
        "\n",
        "losses_data = {\"train\": [], \"test\": []}\n",
        "for iter in tqdm(range(epoch, max_iters)):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        losses_data[\"train\"].append(losses[\"train\"].cpu().numpy())\n",
        "        losses_data[\"test\"].append(losses[\"test\"].cpu().numpy())\n",
        "        print(\n",
        "            f\"Step {iter}, train loss:{losses['train']:.4f}, test loss:{losses['test']:.4f}\"\n",
        "        )\n",
        "\n",
        "    if iter % print_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": iter,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": losses,\n",
        "            },\n",
        "            MODEL_CHECKPOINT.format(iter=iter),\n",
        "        )\n",
        "        losses_data[\"train\"].append(losses[\"train\"].cpu().numpy())\n",
        "        losses_data[\"test\"].append(losses[\"test\"].cpu().numpy())\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Generate from the model:\n",
        "            output = m.generate(\n",
        "                torch.zeros((1, 2), dtype=torch.long).to(device).contiguous(), 1000\n",
        "            )[0].tolist()\n",
        "\n",
        "        print(\n",
        "            f\"Step {iter}, train loss:{losses['train']:.4f}, test loss:{losses['test']:.4f}\"\n",
        "        )\n",
        "        model.train()\n",
        "\n",
        "    # Get data\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # Evaluate loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"./model.pt\")\n",
        "# Generate from the model:\n",
        "output = m.generate(torch.zeros((1, 2), dtype=torch.long).to(device), 1000)[0].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPKr7e0sA_cg"
      },
      "id": "IPKr7e0sA_cg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}